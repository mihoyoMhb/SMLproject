{
 "cells": [
  {
   "cell_type": "code",
   "id": "4877ba83d2d1d4dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T17:21:21.195331Z",
     "start_time": "2024-12-06T17:20:34.856751Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# 按照sklearn的官方代码写法，这里先封装一个自定义的预处理器（处理步骤依旧和统一的结果一样）\n",
    "\"\"\"\n",
    "    写法就是，先必须声明一个自己的preprocessor，里面必须包含fit和transform函数,整体必须是一个类。\n",
    "    同时注意在tuning函数里面，写上一个如下的玩意\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', CustomPreprocessor()),\n",
    "        ('classifier', 你的模型(random_state=42))\n",
    "    ])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def label_weather_cluster(cluster_label):\n",
    "    if cluster_label == 1:\n",
    "        return 'bad_weather'\n",
    "    elif cluster_label == 0:\n",
    "        return 'good_weather'\n",
    "    elif cluster_label == 2:\n",
    "        return 'neutral_weather'\n",
    "\n",
    "\n",
    "class CustomPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "        self.weather_dummies_columns = None\n",
    "        self.categorical_features = None\n",
    "        self.numeric_features = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :return: self\n",
    "        不知道为什么不能删掉这一堆，删掉就报错，还是按照官方指南来写吧，可能是最后fit模型的时候\n",
    "        对于测试集数据也要做一次类似的变换\n",
    "        这里的思路是，我们需要\n",
    "        \"\"\"\n",
    "        # 对数变换\n",
    "        X = X.copy()\n",
    "        X['visibility'] = np.log1p(X['visibility'])\n",
    "        X['snowdepth'] = np.log1p(X['snowdepth'])\n",
    "        X['precip'] = np.log1p(X['precip'])\n",
    "        # 时间特征转换\n",
    "        X['month_sin'] = np.sin(2 * np.pi * X['month'] / 12)\n",
    "        X['month_cos'] = np.cos(2 * np.pi * X['month'] / 12)\n",
    "        X['day_of_week_sin'] = np.sin(2 * np.pi * X['day_of_week'] / 7)\n",
    "        X['day_of_week_cos'] = np.cos(2 * np.pi * X['day_of_week'] / 7)\n",
    "        X['hour_of_day_sin'] = np.sin(2 * np.pi * X['hour_of_day'] / 24)\n",
    "        X['hour_of_day_cos'] = np.cos(2 * np.pi * X['hour_of_day'] / 24)\n",
    "\n",
    "        X = X.drop(columns=['month', 'day_of_week', 'hour_of_day'], axis=1)\n",
    "\n",
    "        clustering_features = ['temp', 'dew', 'humidity', 'snowdepth', 'windspeed', 'cloudcover',\n",
    "                               'visibility', 'precip', 'snow']\n",
    "        # 在训练数据上拟合 KMeans\n",
    "        self.kmeans.fit(X[clustering_features])\n",
    "        # 添加聚类结果\n",
    "        X['weather_cluster'] = self.kmeans.labels_\n",
    "        # 映射聚类标签到天气质量\n",
    "        X['weather_quality'] = X['weather_cluster'].apply(label_weather_cluster)\n",
    "        # One-Hot 编码\n",
    "        weather_dummies = pd.get_dummies(X['weather_quality'], prefix='weather', drop_first=True)\n",
    "        # 保存天气哑变量的列名\n",
    "        self.weather_dummies_columns = weather_dummies.columns\n",
    "        X = pd.concat([X, weather_dummies], axis=1)\n",
    "        # 选择特征\n",
    "        weather_features = ['temp', 'dew', 'humidity', 'snowdepth', 'windspeed', 'cloudcover',\n",
    "                            'visibility', 'precip', 'snow']\n",
    "        self.categorical_features = ['holiday', 'weekday'] + list(self.weather_dummies_columns)\n",
    "        self.numeric_features = [col for col in X.columns if\n",
    "                                 col not in ['holiday', 'weekday', 'increase_stock', 'weather_cluster',\n",
    "                                             'weather_quality'] + list(self.weather_dummies_columns) + weather_features]\n",
    "        # 转换布尔类型\n",
    "        for col in X.select_dtypes(include=['bool']).columns:\n",
    "            X[col] = X[col].astype(int)\n",
    "        # 在训练数据上拟合 StandardScaler\n",
    "        self.scaler.fit(X[self.numeric_features])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"这个函数看似重复，但是不能动\"\"\"\n",
    "        X = X.copy()\n",
    "\n",
    "        X['visibility'] = np.log1p(X['visibility'])\n",
    "        X['snowdepth'] = np.log1p(X['snowdepth'])\n",
    "        X['precip'] = np.log1p(X['precip'])\n",
    "\n",
    "        X['month_sin'] = np.sin(2 * np.pi * X['month'] / 12)\n",
    "        X['month_cos'] = np.cos(2 * np.pi * X['month'] / 12)\n",
    "        X['day_of_week_sin'] = np.sin(2 * np.pi * X['day_of_week'] / 7)\n",
    "        X['day_of_week_cos'] = np.cos(2 * np.pi * X['day_of_week'] / 7)\n",
    "        X['hour_of_day_sin'] = np.sin(2 * np.pi * X['hour_of_day'] / 24)\n",
    "        X['hour_of_day_cos'] = np.cos(2 * np.pi * X['hour_of_day'] / 24)\n",
    "\n",
    "        X = X.drop(columns=['month', 'day_of_week', 'hour_of_day'], axis=1)\n",
    "\n",
    "        clustering_features = ['temp', 'dew', 'humidity', 'snowdepth', 'windspeed', 'cloudcover',\n",
    "                               'visibility', 'precip', 'snow']\n",
    "\n",
    "        X['weather_cluster'] = self.kmeans.predict(X[clustering_features])\n",
    "\n",
    "        X['weather_quality'] = X['weather_cluster'].apply(label_weather_cluster)\n",
    "\n",
    "        weather_dummies = pd.get_dummies(X['weather_quality'], prefix='weather', drop_first=True)\n",
    "\n",
    "        for col in self.weather_dummies_columns:\n",
    "            if col not in weather_dummies.columns:\n",
    "                weather_dummies[col] = 0\n",
    "        X = pd.concat([X, weather_dummies], axis=1)\n",
    "\n",
    "        for col in X.select_dtypes(include=['bool']).columns:\n",
    "            X[col] = X[col].astype(int)\n",
    "\n",
    "        X_scaled = self.scaler.transform(X[self.numeric_features])\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=self.numeric_features, index=X.index)\n",
    "\n",
    "        X_processed = pd.concat([X[self.categorical_features], X_scaled], axis=1)\n",
    "        return X_processed\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=['low_bike_demand', 'high_bike_demand'])\n",
    "    return accuracy, f1, report\n",
    "\n",
    "\n",
    "def tune_random_forest_rs(X_train, y_train, cv=10, scoring='f1', n_iter=100):\n",
    "    num_pos = np.sum(y_train == 1)\n",
    "    num_neg = np.sum(y_train == 0)\n",
    "    ratio = num_neg / num_pos\n",
    "    param_dist = {\n",
    "        'classifier__n_estimators': [2 * i for i in range(250, 300)],\n",
    "        'classifier__max_depth': list(range(15, 25)),\n",
    "        'classifier__min_samples_split': list(range(3, 32)),\n",
    "        'classifier__min_samples_leaf': list(range(3, 32)),\n",
    "        'classifier__max_features': ['sqrt', 'log2', 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        'classifier__bootstrap': [True, False],\n",
    "        'classifier__class_weight': ['balanced', {0: 1, 1: ratio}, {0: 1 / ratio, 1: 1}, {0: 1, 1: 1}],\n",
    "        'classifier__criterion': [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        'classifier__warm_start': [False],\n",
    "    }\n",
    "    # 创建 Pipeline\n",
    "    \"\"\"这里也是最关键的步骤，需要这里加入pipeline\"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', CustomPreprocessor()),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train.ravel())\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "\n",
    "def tune_ada_boost(X_train, y_train, cv=10, scoring='f1', n_iter=10):\n",
    "    param_dist = {\n",
    "        'classifier__n_estimators': [10 * i for i in range(10, 101)],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2, 0.3, 0,4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "        'classifier__algorithm': ['SAMME']\n",
    "    }\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', CustomPreprocessor()),\n",
    "        ('classifier', AdaBoostClassifier(random_state=42))\n",
    "    ])\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train.ravel())\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    LDA和QDA有点奇怪，但是先这样写吧\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# def train_lda(X_train, y_train):\n",
    "#     pipeline = Pipeline([\n",
    "#         ('preprocessor', CustomPreprocessor()),\n",
    "#         ('classifier', LinearDiscriminantAnalysis())\n",
    "#     ])\n",
    "#     pipeline.fit(X_train, y_train.ravel())\n",
    "#     return pipeline\n",
    "# \n",
    "# \n",
    "# def train_qda(X_train, y_train):\n",
    "#     pipeline = Pipeline([\n",
    "#         ('preprocessor', CustomPreprocessor()),\n",
    "#         ('classifier', QuadraticDiscriminantAnalysis())\n",
    "#     ])\n",
    "#     pipeline.fit(X_train, y_train.ravel())\n",
    "#     return pipeline\n",
    "def train_lda(X_train, y_train):\n",
    "    param_dist = {\n",
    "        'classifier__solver': ['lsqr', 'eigen'],  \n",
    "        'classifier__shrinkage': [None, 'auto', 0.1, 0.5],\n",
    "    }\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', CustomPreprocessor()),\n",
    "        ('classifier', LinearDiscriminantAnalysis())\n",
    "    ])\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train.ravel())\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "\n",
    "def train_qda(X_train, y_train):\n",
    "    # 对QDA进行超参数搜索\n",
    "    param_dist = {\n",
    "        'classifier__reg_param': [0.0, 0.1, 0.3, 0.5, 0.7, 0.9], \n",
    "        'classifier__store_covariance': [True, False],\n",
    "        'classifier__tol': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    }\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', CustomPreprocessor()),\n",
    "        ('classifier', QuadraticDiscriminantAnalysis())\n",
    "    ])\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train.ravel())\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "def tune_logistic_regression(X_train, y_train, cv=10, scoring='f1', n_iter=10):\n",
    "    num_pos = np.sum(y_train == 1)\n",
    "    num_neg = np.sum(y_train == 0)\n",
    "    ratio = num_neg / num_pos\n",
    "    param_dist = {\n",
    "        'classifier__C': np.logspace(-3, 3, 7),\n",
    "        'classifier__penalty': ['l2'],\n",
    "        'classifier__class_weight': ['balanced', {0: 1, 1: ratio}],\n",
    "        'classifier__solver': ['lbfgs', 'liblinear'],\n",
    "        'classifier__max_iter': [100, 500, 1000],\n",
    "    }\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', CustomPreprocessor()),\n",
    "        ('classifier', LogisticRegression(random_state=42))\n",
    "    ])\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train.ravel())\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "\n",
    "def tune_knn(X_train, y_train, cv=10, scoring='f1', n_iter=10):\n",
    "    param_dist = {\n",
    "        'classifier__n_neighbors': list(range(1, 31)),\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'classifier__p': [1, 2],  # p=1 曼哈顿距离，p=2 欧氏距离\n",
    "    }\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', CustomPreprocessor()),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    random_search.fit(X_train, y_train.ravel())\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据, 文件地址自己改一下\n",
    "    data = pd.read_csv('data/training_data_fall2024.csv')\n",
    "    # models\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'LDA': LinearDiscriminantAnalysis(),\n",
    "        'QDA': QuadraticDiscriminantAnalysis(),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=42)\n",
    "    }\n",
    "    # 定义特征和目标变量\n",
    "    X_all = data.copy()\n",
    "    y_all = data['increase_stock'].map({'low_bike_demand': 0, 'high_bike_demand': 1}).to_numpy().ravel()\n",
    "    X_all = X_all.drop(columns=['increase_stock'])\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42,\n",
    "                                                              stratify=y_all)\n",
    "    # 存储结果\n",
    "    results = {}\n",
    "\n",
    "    # 训练 Random Forest\n",
    "    print(\"\\nTraining Random Forest...\")\n",
    "    best_rf = tune_random_forest_rs(X_trainval, y_trainval)\n",
    "    y_pred_rf = best_rf.predict(X_test)\n",
    "    accuracy_rf, f1_rf, report_rf = evaluate_model(y_test, y_pred_rf)\n",
    "    print(f\"Random Forest Test Set Accuracy: {accuracy_rf:.2f}\")\n",
    "    print(f\"Random Forest Test Set F1 Score: {f1_rf:.2f}\")\n",
    "    print(f\"Random Forest Classification Report:\\n{report_rf}\")\n",
    "    print(f\"Random Forest Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_rf)}\")\n",
    "    results['Random Forest'] = {\n",
    "        'model': best_rf,\n",
    "        'accuracy': accuracy_rf,\n",
    "        'f1': f1_rf,\n",
    "        'report': report_rf,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred_rf)\n",
    "    }\n",
    "\n",
    "    # 训练 Logistic Regression\n",
    "    print(\"\\nTraining Logistic Regression...\")\n",
    "    best_lr = tune_logistic_regression(X_trainval, y_trainval)\n",
    "    y_pred_lr = best_lr.predict(X_test)\n",
    "    accuracy_lr, f1_lr, report_lr = evaluate_model(y_test, y_pred_lr)\n",
    "    print(f\"Logistic Regression Test Set Accuracy: {accuracy_lr:.2f}\")\n",
    "    print(f\"Logistic Regression Test Set F1 Score: {f1_lr:.2f}\")\n",
    "    print(f\"Logistic Regression Classification Report:\\n{report_lr}\")\n",
    "    print(f\"Logistic Regression Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_lr)}\")\n",
    "    results['Logistic Regression'] = {\n",
    "        'model': best_lr,\n",
    "        'accuracy': accuracy_lr,\n",
    "        'f1': f1_lr,\n",
    "        'report': report_lr,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred_lr)\n",
    "    }\n",
    "\n",
    "    # 训练 AdaBoost\n",
    "    print(\"\\nTraining AdaBoost...\")\n",
    "    best_ada = tune_ada_boost(X_trainval, y_trainval)\n",
    "    y_pred_ada = best_ada.predict(X_test)\n",
    "    accuracy_ada, f1_ada, report_ada = evaluate_model(y_test, y_pred_ada)\n",
    "    print(f\"AdaBoost Test Set Accuracy: {accuracy_ada:.2f}\")\n",
    "    print(f\"AdaBoost Test Set F1 Score: {f1_ada:.2f}\")\n",
    "    print(f\"AdaBoost Classification Report:\\n{report_ada}\")\n",
    "    print(f\"AdaBoost Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_ada)}\")\n",
    "    results['AdaBoost'] = {\n",
    "        'model': best_ada,\n",
    "        'accuracy': accuracy_ada,\n",
    "        'f1': f1_ada,\n",
    "        'report': report_ada,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred_ada)\n",
    "    }\n",
    "\n",
    "    # 训练 LDA\n",
    "    print(\"\\nTraining LDA...\")\n",
    "    best_lda = train_lda(X_trainval, y_trainval)\n",
    "    y_pred_lda = best_lda.predict(X_test)\n",
    "    accuracy_lda, f1_lda, report_lda = evaluate_model(y_test, y_pred_lda)\n",
    "    print(f\"LDA Test Set Accuracy: {accuracy_lda:.2f}\")\n",
    "    print(f\"LDA Test Set F1 Score: {f1_lda:.2f}\")\n",
    "    print(f\"LDA Classification Report:\\n{report_lda}\")\n",
    "    print(f\"LDA Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_lda)}\")\n",
    "    results['LDA'] = {\n",
    "        'model': best_lda,\n",
    "        'accuracy': accuracy_lda,\n",
    "        'f1': f1_lda,\n",
    "        'report': report_lda,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred_lda)\n",
    "    }\n",
    "\n",
    "    # 训练 QDA\n",
    "    print(\"\\nTraining QDA...\")\n",
    "    best_qda = train_qda(X_trainval, y_trainval)\n",
    "    y_pred_qda = best_qda.predict(X_test)\n",
    "    accuracy_qda, f1_qda, report_qda = evaluate_model(y_test, y_pred_qda)\n",
    "    print(f\"QDA Test Set Accuracy: {accuracy_qda:.2f}\")\n",
    "    print(f\"QDA Test Set F1 Score: {f1_qda:.2f}\")\n",
    "    print(f\"QDA Classification Report:\\n{report_qda}\")\n",
    "    print(f\"QDA Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_qda)}\")\n",
    "    results['QDA'] = {\n",
    "        'model': best_qda,\n",
    "        'accuracy': accuracy_qda,\n",
    "        'f1': f1_qda,\n",
    "        'report': report_qda,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred_qda)\n",
    "    }\n",
    "\n",
    "    print(\"\\nTraining KNN...\")\n",
    "    best_knn = tune_knn(X_trainval, y_trainval)\n",
    "    y_pred_knn = best_knn.predict(X_test)\n",
    "    accuracy_knn, f1_knn, report_knn = evaluate_model(y_test, y_pred_knn)\n",
    "    print(f\"KNN Test Set Accuracy: {accuracy_knn:.2f}\")\n",
    "    print(f\"KNN Test Set F1 Score: {f1_knn:.2f}\")\n",
    "    print(f\"KNN Classification Report:\\n{report_knn}\")\n",
    "    print(f\"KNN Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_knn)}\")\n",
    "    results['KNN'] = {\n",
    "        'model': best_knn,\n",
    "        'accuracy': accuracy_knn,\n",
    "        'f1': f1_knn,\n",
    "        'report': report_knn,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred_knn)\n",
    "    }\n",
    "\n",
    "    # 输出所有模型的结果\n",
    "    print(\"\\nSummary of Model Performance:\")\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"{model_name} Test Set Accuracy: {result['accuracy']:.2f}\")\n",
    "        print(f\"{model_name} Test Set F1 Score: {result['f1']:.2f}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test Set Accuracy: 0.83\n",
      "Random Forest Test Set F1 Score: 0.64\n",
      "Random Forest Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " low_bike_demand       0.96      0.83      0.89       262\n",
      "high_bike_demand       0.52      0.83      0.64        58\n",
      "\n",
      "        accuracy                           0.83       320\n",
      "       macro avg       0.74      0.83      0.76       320\n",
      "    weighted avg       0.88      0.83      0.84       320\n",
      "\n",
      "Random Forest Confusion Matrix:\n",
      "[[217  45]\n",
      " [ 10  48]]\n",
      "\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Set Accuracy: 0.78\n",
      "Logistic Regression Test Set F1 Score: 0.58\n",
      "Logistic Regression Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " low_bike_demand       0.95      0.77      0.85       262\n",
      "high_bike_demand       0.44      0.83      0.58        58\n",
      "\n",
      "        accuracy                           0.78       320\n",
      "       macro avg       0.70      0.80      0.72       320\n",
      "    weighted avg       0.86      0.78      0.80       320\n",
      "\n",
      "Logistic Regression Confusion Matrix:\n",
      "[[202  60]\n",
      " [ 10  48]]\n",
      "\n",
      "Training AdaBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Test Set Accuracy: 0.90\n",
      "AdaBoost Test Set F1 Score: 0.69\n",
      "AdaBoost Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " low_bike_demand       0.91      0.97      0.94       262\n",
      "high_bike_demand       0.83      0.59      0.69        58\n",
      "\n",
      "        accuracy                           0.90       320\n",
      "       macro avg       0.87      0.78      0.81       320\n",
      "    weighted avg       0.90      0.90      0.90       320\n",
      "\n",
      "AdaBoost Confusion Matrix:\n",
      "[[255   7]\n",
      " [ 24  34]]\n",
      "\n",
      "Training LDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:318: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Test Set Accuracy: 0.87\n",
      "LDA Test Set F1 Score: 0.63\n",
      "LDA Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " low_bike_demand       0.91      0.93      0.92       262\n",
      "high_bike_demand       0.66      0.60      0.63        58\n",
      "\n",
      "        accuracy                           0.87       320\n",
      "       macro avg       0.79      0.77      0.78       320\n",
      "    weighted avg       0.87      0.87      0.87       320\n",
      "\n",
      "LDA Confusion Matrix:\n",
      "[[244  18]\n",
      " [ 23  35]]\n",
      "\n",
      "Training QDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Test Set Accuracy: 0.88\n",
      "QDA Test Set F1 Score: 0.67\n",
      "QDA Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " low_bike_demand       0.93      0.93      0.93       262\n",
      "high_bike_demand       0.67      0.67      0.67        58\n",
      "\n",
      "        accuracy                           0.88       320\n",
      "       macro avg       0.80      0.80      0.80       320\n",
      "    weighted avg       0.88      0.88      0.88       320\n",
      "\n",
      "QDA Confusion Matrix:\n",
      "[[243  19]\n",
      " [ 19  39]]\n",
      "\n",
      "Training KNN...\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "KNN Test Set Accuracy: 0.89\n",
      "KNN Test Set F1 Score: 0.65\n",
      "KNN Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " low_bike_demand       0.91      0.95      0.93       262\n",
      "high_bike_demand       0.74      0.59      0.65        58\n",
      "\n",
      "        accuracy                           0.89       320\n",
      "       macro avg       0.83      0.77      0.79       320\n",
      "    weighted avg       0.88      0.89      0.88       320\n",
      "\n",
      "KNN Confusion Matrix:\n",
      "[[250  12]\n",
      " [ 24  34]]\n",
      "\n",
      "Summary of Model Performance:\n",
      "Random Forest Test Set Accuracy: 0.83\n",
      "Random Forest Test Set F1 Score: 0.64\n",
      "\n",
      "Logistic Regression Test Set Accuracy: 0.78\n",
      "Logistic Regression Test Set F1 Score: 0.58\n",
      "\n",
      "AdaBoost Test Set Accuracy: 0.90\n",
      "AdaBoost Test Set F1 Score: 0.69\n",
      "\n",
      "LDA Test Set Accuracy: 0.87\n",
      "LDA Test Set F1 Score: 0.63\n",
      "\n",
      "QDA Test Set Accuracy: 0.88\n",
      "QDA Test Set F1 Score: 0.67\n",
      "\n",
      "KNN Test Set Accuracy: 0.89\n",
      "KNN Test Set F1 Score: 0.65\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
